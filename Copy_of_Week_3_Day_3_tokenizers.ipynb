{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stephennacion06/llm_engineering/blob/main/Copy_of_Week_3_Day_3_tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizers\n",
        "\n",
        "For this Colab session, we explore the world of Tokenizers\n",
        "\n",
        "You can run this notebook on a free CPU, or locally on your box if you prefer.\n"
      ],
      "metadata": {
        "id": "ayiYEglO9T8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reminder: 2 important pro-tips for using Colab:\n",
        "\n",
        "**Pro-tip 1:**\n",
        "\n",
        "Don't worry about warnings and messages!\n",
        "\n",
        "**Pro-tip 2:**\n",
        "\n",
        "In the middle of running a Colab, you might get an error like this:\n",
        "\n",
        "> Runtime error: CUDA is required but not available for bitsandbytes. Please consider installing [...]\n",
        "\n",
        "This is a super-misleading error message! Please don't try changing versions of packages...\n",
        "\n",
        "This actually happens because Google has switched out your Colab runtime, perhaps because Google Colab was too busy. The solution is:\n",
        "\n",
        "1. Kernel menu >> Disconnect and delete runtime\n",
        "2. Reload the colab from fresh and Edit menu >> Clear All Outputs\n",
        "3. Connect to a new T4 using the button at the top right\n",
        "4. Select \"View resources\" from the menu on the top right to confirm you have a GPU\n",
        "5. Rerun the cells in the colab, from the top down, starting with the pip installs\n",
        "\n",
        "And all should work great - otherwise, ask me!"
      ],
      "metadata": {
        "id": "bK1rDJXe2KjS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9zvDGWD5pKp"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sign in to Hugging Face\n",
        "\n",
        "1. If you haven't already done so, create a free HuggingFace account at https://huggingface.co and navigate to Settings, then Create a new API token, giving yourself write permissions\n",
        "\n",
        "**IMPORTANT** when you create your HuggingFace API key, please be sure to select read/write permissions for your key by clicking on the WRITE tab, otherwise you may get problems later.\n",
        "\n",
        "2. Press the \"key\" icon on the side panel to the left, and add a new secret:\n",
        "`HF_TOKEN = your_token`\n",
        "\n",
        "3. Execute the cell below to log in."
      ],
      "metadata": {
        "id": "xyKWKWSw7Iqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to Hugging Face\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if hf_token and hf_token.startswith(\"hf_\"):\n",
        "  print(\"HF key looks good so far\")\n",
        "else:\n",
        "  print(\"HF key is not set - please click the key in the left sidebar\")\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "\n",
        "# Check Google Colab GPU\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "  if gpu_info.find('Tesla T4') >= 0:\n",
        "    print(\"Success - Connected to a T4\")\n",
        "  else:\n",
        "    print(\"NOT CONNECTED TO A T4\")"
      ],
      "metadata": {
        "id": "xd7cEDUC6Lkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accessing Llama 3.1 from Meta\n",
        "\n",
        "In order to use the fantastic Llama 3.1, Meta does require you to sign their terms of service.\n",
        "\n",
        "Visit their model instructions page in Hugging Face:\n",
        "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n",
        "\n",
        "At the top of the page are instructions on how to agree to their terms. If possible, you should use the same email as your huggingface account.\n",
        "\n",
        "In my experience approval comes in a couple of minutes. Once you've been approved for any 3.1 model, it applies to the whole 3.1 family of models. For whatever reason, occasionally Meta doesn't approve access. If that happens to you, please follow [this](https://colab.research.google.com/drive/1deJO03YZTXUwcq2vzxWbiBhrRuI29Vo8?usp=sharing) troubleshooting.\n",
        "\n",
        "If the next cell gives you an error, then please check:  \n",
        "1. Are you logged in to HuggingFace? Try running `login()` to check your key works\n",
        "2. Did you set up your API key with full read and write permissions?\n",
        "3. If you visit the Llama3.1 page with the link above, does it show that you have access to the model near the top?\n",
        "\n",
        "I've also set up this troubleshooting colab to try to diagnose any HuggingFace connectivity issues:  \n",
        "https://colab.research.google.com/drive/1deJO03YZTXUwcq2vzxWbiBhrRuI29Vo8?usp=sharing\n"
      ],
      "metadata": {
        "id": "ZSiYqPn87msu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "swoiXwUb7RoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "tokens = tokenizer.encode(text)\n",
        "tokens"
      ],
      "metadata": {
        "id": "kgJTmHNm8Ui4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "character_count = len(text)\n",
        "word_count = len(text.split(' '))\n",
        "token_count = len(tokens)\n",
        "print(f\"There are {character_count} characters, {word_count} words and {token_count} tokens\")"
      ],
      "metadata": {
        "id": "QHnp79Ig8vPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokens)"
      ],
      "metadata": {
        "id": "rtptNsDf83RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(tokens)"
      ],
      "metadata": {
        "id": "ZlQT65oz8-aF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer.vocab\n",
        "tokenizer.get_added_vocab()"
      ],
      "metadata": {
        "id": "y7LTUIlD9Gdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.vocab)"
      ],
      "metadata": {
        "id": "byOV6WAGBZZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instruct variants of models\n",
        "\n",
        "Many models have a variant that has been trained for use in Chats.  \n",
        "These are typically labelled with the word \"Instruct\" at the end.  \n",
        "They have been trained to expect prompts with a particular format that includes system, user and assistant prompts.  \n",
        "\n",
        "There is a utility method `apply_chat_template` that will convert from the messages list format we are familiar with, into the right input prompt for this model."
      ],
      "metadata": {
        "id": "kke10gYj_U87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "4DJs4UPx9XfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "uKJjVJ9T_GFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crucial \"Aha\" moment\n",
        "\n",
        "For 2.5 weeks, I've given you the impression that LLMs could receive a list of python dictionaries in some way:\n",
        "\n",
        "```python\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]\n",
        "```\n",
        "\n",
        "But an LLM is just a Data Science model that takes a sequence of numbers and predicts the probability of the next number! You can't pass a bunch of Python objects into a statistical model!\n",
        "\n",
        "### And now you have the missing piece of the puzzle..\n",
        "\n",
        "The messages in OpenAI format get converted:\n",
        "\n",
        "1. ...into a sequence of words with special tags to separate the System, User, Assistant prompt\n",
        "2. ...then the words are broken down into fragments - \"tokens\"\n",
        "3. ...then the tokens are replaced with Token IDs - and this is the input sequence\n",
        "\n",
        "> The input to an LLM is a sequence of Token IDs. The output is the probability distribution of the next Token ID to follow this input.\n",
        "\n",
        "That's it!\n"
      ],
      "metadata": {
        "id": "gEhEtHeXgvlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying new models\n",
        "\n",
        "We will now work with 3 models:\n",
        "\n",
        "Phi4 from Microsoft  \n",
        "DeepSeek 3.1 from DeepSeek AI  \n",
        "QwenCoder 2.5 from Alibaba Cloud"
      ],
      "metadata": {
        "id": "wlmNvvb-AIKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PHI4 = \"microsoft/Phi-4-mini-instruct\"\n",
        "DEEPSEEK = \"deepseek-ai/DeepSeek-V3.1\"\n",
        "QWEN_CODER = \"Qwen/Qwen2.5-Coder-7B-Instruct\""
      ],
      "metadata": {
        "id": "FWUMdt_iAZQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi4_tokenizer = AutoTokenizer.from_pretrained(PHI4)\n",
        "\n",
        "text = \"I am curiously excited to show Hugging Face Tokenizers in action to my LLM engineers\"\n",
        "print(\"Llama:\")\n",
        "tokens = tokenizer.encode(text)\n",
        "print(tokens)\n",
        "print(tokenizer.batch_decode(tokens))\n",
        "print(\"\\nPhi 4:\")\n",
        "tokens = phi4_tokenizer.encode(text)\n",
        "print(tokens)\n",
        "print(phi4_tokenizer.batch_decode(tokens))\n"
      ],
      "metadata": {
        "id": "WM9tU_ZnAbkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Llama:\")\n",
        "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "print(\"\\nPhi 4:\")\n",
        "print(phi4_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
      ],
      "metadata": {
        "id": "4CrdGSBZAxx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deepseek_tokenizer = AutoTokenizer.from_pretrained(DEEPSEEK)\n",
        "\n",
        "text = \"I am curiously excited to show Hugging Face Tokenizers in action to my LLM engineers\"\n",
        "print(tokenizer.encode(text))\n",
        "print()\n",
        "print(phi4_tokenizer.encode(text))\n",
        "print()\n",
        "print(deepseek_tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "yr16p4HSA2A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Llama:\")\n",
        "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "print(\"\\nPhi:\")\n",
        "print(phi4_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "print(\"\\nDeepSeek:\")\n",
        "print(deepseek_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
      ],
      "metadata": {
        "id": "sQ5wFS1oBdEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qwen_tokenizer = AutoTokenizer.from_pretrained(QWEN_CODER)\n",
        "code = \"\"\"\n",
        "def hello_world(person):\n",
        "  print(\"Hello\", person)\n",
        "\"\"\"\n",
        "tokens = qwen_tokenizer.encode(code)\n",
        "for token in tokens:\n",
        "  print(f\"{token}={qwen_tokenizer.decode(token)}\")"
      ],
      "metadata": {
        "id": "_GGe6hzSBkBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dmmDkEjG9Rkz"
      }
    }
  ]
}