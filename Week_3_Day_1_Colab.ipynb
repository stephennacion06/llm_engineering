{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stephennacion06/llm_engineering/blob/main/Week_3_Day_1_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 3 Day 1\n",
        "\n",
        "Welcome to Google Colab - using free and powerful compute in the cloud.\n",
        "\n",
        "Google Colab gives you a remote Notebook-style browser window on to a machine, and you run code \"locally\" on that machine.\n",
        "\n",
        "## Benefits of Colab\n",
        "\n",
        "1. Free access to T4 GPUs!\n",
        "2. Easy ability to share code and collaborate on it\n",
        "3. Everyone gets to use identical code - no environment differences\n",
        "\n",
        "## Downsides of Colab\n",
        "\n",
        "1. As it's free, Google reserves the right to bump you off the box at any point (\"reset the runtime\"), and this happens most quickly if no code is running. They can also downgrade you from a T4 to a CPU-only box. Sometimes this happens silently. You need to start everything again from the top. Paid plans last longer.\n",
        "2. You need to pip install packages every single time (but no pip installs today)\n",
        "3. There's some latency - it's not as interactive as coding on your own box\n",
        "\n",
        "## Survival Guide\n",
        "\n",
        "1. Always start by pressing the drop down arrow by Connect on the top right, and \"Connect to a hosted runtime: T4\"\n",
        "2. From that dropdown, \"View Resources\" to check you have a GPU and monitor memory\n",
        "3. If things go awry, Runtime >> Disconnect and Delete Runtime, Connect again, and then run cells from the top\n",
        "4. Always run the pip installs! Ignore pip dependency errors.\n",
        "\n",
        "Runtime >> Restart session: this restarts the Python Kernel, but pip packages remain installed and the disk remains the same.\n",
        "\n",
        "Runtime >> Diconnect and delete session. This wipes everything."
      ],
      "metadata": {
        "id": "JTygxy-RAn1f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iQqYgGVYnhco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the GPU - it should be a Tesla T4\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "  if gpu_info.find('Tesla T4') >= 0:\n",
        "    print(\"Success - Connected to a T4\")\n",
        "  else:\n",
        "    print(\"NOT CONNECTED TO A T4\")"
      ],
      "metadata": {
        "id": "E2aO6PbB0WU3",
        "outputId": "ebb723de-bf5b-4394-9447-d5da68f363de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct  8 05:38:46 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Success - Connected to a T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connecting Hugging Face\n",
        "\n",
        "You'll need to log in to the HuggingFace hub if you've not done so before.\n",
        "\n",
        "1. If you haven't already done so, create a **free** HuggingFace account at https://huggingface.co and navigate to Settings from the user menu on the top right. Then Create a new API token, giving yourself write permissions.  \n",
        "\n",
        "**IMPORTANT** when you create your HuggingFace API key, please be sure to select WRITE permissions for your key by clicking on the WRITE tab, otherwise you may get problems later. Not \"fine-grained\" but \"write\".\n",
        "\n",
        "2. Back here in colab, press the \"key\" icon on the side panel to the left, and add a new secret:  \n",
        "  In the name field put `HF_TOKEN`  \n",
        "  In the value field put your actual token: `hf_...`  \n",
        "  Ensure the notebook access switch is turned ON.\n",
        "\n",
        "3. Execute the cell below to log in. You'll need to do this on each of your colabs. It's a really useful way to manage your secrets without needing to type them into colab."
      ],
      "metadata": {
        "id": "TV8_hr1rCGUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "ZR-wgFH-CKtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "from diffusers import AutoPipelineForText2Image\n",
        "import torch\n",
        "\n",
        "pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
        "pipe.to(\"cuda\")\n",
        "prompt = \"A class of students learning AI engineering in a vibrant pop-art style\"\n",
        "image = pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
        "display(image)\n"
      ],
      "metadata": {
        "id": "9jTthxWyAJJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart the kernel\n",
        "\n",
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "HAeRMxVdJMDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
        "pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"A class of data scientists learning AI engineering in a vibrant high-energy pop-art style\"\n",
        "\n",
        "image = pipe(prompt=prompt, num_inference_steps=30).images[0]\n",
        "\n",
        "display(image)\n"
      ],
      "metadata": {
        "id": "UubQ06ZvEOj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart the kernel\n",
        "\n",
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "ZeE9YBDoXyvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "base = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True)\n",
        "base.to(\"cuda\")\n",
        "refiner = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-refiner-1.0\", text_encoder_2=base.text_encoder_2, vae=base.vae, torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\",)\n",
        "refiner.to(\"cuda\")\n",
        "\n",
        "# Define how many steps and what % of steps to be run on each experts (80/20) here\n",
        "n_steps = 40\n",
        "high_noise_frac = 0.8\n",
        "\n",
        "prompt = \"A class of data scientists learning AI engineering in a vibrant high-energy pop-art style\"\n",
        "\n",
        "# run both experts\n",
        "image = base(\n",
        "    prompt=prompt,\n",
        "    num_inference_steps=n_steps,\n",
        "    denoising_end=high_noise_frac,\n",
        "    output_type=\"latent\",\n",
        ").images\n",
        "\n",
        "image = refiner(\n",
        "    prompt=prompt,\n",
        "    num_inference_steps=n_steps,\n",
        "    denoising_start=high_noise_frac,\n",
        "    image=image,\n",
        ").images[0]\n",
        "\n",
        "display(image)"
      ],
      "metadata": {
        "id": "Lmijo03lNsCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(image)"
      ],
      "metadata": {
        "id": "6LaVxbaUXrHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "WEQ2eeW7Kaxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets==3.6.0"
      ],
      "metadata": {
        "id": "fYqEcdsPxmBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "import torch\n",
        "from IPython.display import Audio\n",
        "\n",
        "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device='cuda')\n",
        "embeddings_dataset = load_dataset(\"matthijs/cmu-arctic-xvectors\", split=\"validation\", trust_remote_code=True)\n",
        "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "speech = synthesiser(\"Hi to an artificial intelligence engineer, on the way to mastery!\", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
        "\n",
        "Audio(speech[\"audio\"], rate=speech[\"sampling_rate\"])"
      ],
      "metadata": {
        "id": "c5G9d2XLNo1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "0Vk0La6POTcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This next cells will only work on a powerful GPU box like an A100\n",
        "\n",
        "This is not available on a free T4 box.\n",
        "\n",
        "I just want to show off what's possible with a small paid budget..\n",
        "\n",
        "Rough pricing:\n",
        "\n",
        "- $9.99 = 100 compute units\n",
        "- An A100 = 5.37 compute units per hour as of Oct 2025 (for me)"
      ],
      "metadata": {
        "id": "yv1Iv0vjPlI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the GPU - it should be an A100\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "  if gpu_info.find('A100') >= 0:\n",
        "    print(\"Success - Connected to an NVIDIA A100\")\n",
        "  else:\n",
        "    print(\"NOT CONNECTED TO AN A100\")"
      ],
      "metadata": {
        "id": "bYoVlIwb5YEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import FluxPipeline\n",
        "from IPython.display import display\n",
        "from datetime import datetime\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
        "generator = torch.Generator(device=\"cuda\").manual_seed(0)\n",
        "prompt = \"A class of data scientists learning AI engineering in a vibrant high-energy pop-art style\"\n",
        "\n",
        "# Generate the image using the GPU\n",
        "image = pipe(\n",
        "    prompt,\n",
        "    guidance_scale=0.0,\n",
        "    num_inference_steps=4,\n",
        "    max_sequence_length=256,\n",
        "    generator=generator\n",
        ").images[0]\n",
        "\n",
        "display(image)\n",
        "\n",
        "stop = datetime.now()\n"
      ],
      "metadata": {
        "id": "qw_WV7SgTWBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cost estimate\n",
        "\n",
        "seconds = (stop-start).total_seconds()\n",
        "units_per_hour = 5.37\n",
        "estimated_units = (5.37 / 3600) * seconds\n",
        "estimated_cost = estimated_units * (9.99/100)\n",
        "print(f\"This took {seconds:.1f} seconds and cost an estimated ${estimated_cost:.3f}\")\n",
        "\n",
        "# But there's a catch - you pay for all the time the kernel is active, not just while it's actually calculating!\n",
        "# So remember to shut down a Paid kernel.."
      ],
      "metadata": {
        "id": "pE7Fcljg56x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ysVs1HD0yyD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}