{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Installation Required\n",
    "\n",
    "Before running this notebook, you need to install the Google Generative AI package. Run the cell below **only once**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Day 3 - Conversational AI with Gemini - aka Chatbot!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this notebook, you'll learn how to:\n",
    "1. **Build a conversational chatbot** using Google's Gemini model\n",
    "2. **Maintain conversation history** so the AI remembers context\n",
    "3. **Use system instructions** to guide the AI's behavior and personality\n",
    "4. **Stream responses** for a better user experience\n",
    "5. **Create a web interface** with Gradio for easy interaction\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Conversation History\n",
    "Unlike simple one-off questions, chatbots need to remember what was said before. We'll manage this history to create natural conversations.\n",
    "\n",
    "### System Instructions\n",
    "These are special instructions that guide how the AI behaves - like giving it a role or personality. Think of it as \"behind the scenes\" instructions the user doesn't see.\n",
    "\n",
    "### Streaming\n",
    "Instead of waiting for the complete response, we'll show words as they're generated - just like ChatGPT does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# \n",
    "# os: to access environment variables\n",
    "# dotenv: to load API keys from .env file\n",
    "# google.genai: Google's Gemini AI library (newer SDK)\n",
    "# gradio: for creating web interfaces\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Before we can use Gemini, we need to:\n",
    "1. Load our API key from the `.env` file\n",
    "2. Verify the key is properly set\n",
    "3. Configure the Gemini library to use our key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load and verify API key\n",
    "#\n",
    "# The load_dotenv() function reads your .env file and makes the variables available\n",
    "# We print the first 8 characters to verify it's loaded (never print the full key!)\n",
    "\n",
    "load_dotenv(override=True)\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"✓ Google API Key loaded successfully (begins with: {google_api_key[:8]})\")\n",
    "    # Create the Gemini client\n",
    "    client = genai.Client(api_key=google_api_key)\n",
    "else:\n",
    "    print(\"✗ Google API Key not found - please check your .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Choose Your Model\n",
    "\n",
    "Google offers several Gemini models:\n",
    "- **gemini-2.0-flash-exp**: Latest experimental model, very fast and capable\n",
    "- **gemini-1.5-flash**: Stable, fast, and cost-effective\n",
    "- **gemini-1.5-pro**: Most capable but slower and more expensive\n",
    "\n",
    "We'll use `gemini-1.5-flash` for a good balance of speed and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize the model\n",
    "#\n",
    "# We're choosing gemini-2.0-flash-exp for:\n",
    "# - Latest features\n",
    "# - Fast responses\n",
    "# - Good quality for most tasks\n",
    "\n",
    "MODEL = 'gemini-2.0-flash-exp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## System Instructions - The AI's Personality\n",
    "\n",
    "System instructions are like giving the AI a \"role\" to play. They:\n",
    "- Define how the AI should behave\n",
    "- Set the tone and style of responses\n",
    "- Provide context about what the AI is helping with\n",
    "- Are invisible to the user\n",
    "\n",
    "**Important**: System instructions apply to the ENTIRE conversation, not just one message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define system instructions\n",
    "#\n",
    "# Start with a simple helpful assistant\n",
    "# We'll make this more sophisticated later!\n",
    "\n",
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Building the Chat Function\n",
    "\n",
    "### How Gradio ChatInterface Works\n",
    "\n",
    "Gradio's `ChatInterface` expects a function with this signature:\n",
    "```python\n",
    "def chat(message, history):\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **message**: The user's latest message (string)\n",
    "- **history**: Past conversation in Gradio format (list of dictionaries)\n",
    "\n",
    "### Gradio's History Format\n",
    "\n",
    "Gradio passes history like this:\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! How can I help?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I don't have weather data...\"}\n",
    "]\n",
    "```\n",
    "\n",
    "### Converting to Gemini Format\n",
    "\n",
    "Gemini needs a slightly different format with \"parts\":\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"user\", \"parts\": \"Hello!\"},\n",
    "    {\"role\": \"model\", \"parts\": \"Hi! How can I help?\"}  # Note: \"model\" not \"assistant\"\n",
    "]\n",
    "```\n",
    "\n",
    "### Why We Need Streaming\n",
    "\n",
    "Streaming means we show the response word-by-word as it's generated, rather than waiting for the complete answer. This:\n",
    "- Feels more responsive\n",
    "- Shows progress on long answers\n",
    "- Matches the ChatGPT experience users expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create the chat function\n",
    "#\n",
    "# This function:\n",
    "# 1. Converts Gradio's history format to Gemini's format\n",
    "# 2. Uses the Gemini client to generate responses\n",
    "# 3. Generates a streaming response\n",
    "# 4. Yields each chunk as it arrives (for gradual display)\n",
    "\n",
    "def chat(message, history):\n",
    "    # Convert Gradio format to Gemini format\n",
    "    # Gradio uses: {\"role\": \"assistant\", \"content\": \"text\"}\n",
    "    # Gemini needs: {\"role\": \"model\", \"parts\": [{\"text\": \"text\"}]} for model responses\n",
    "    # and {\"role\": \"user\", \"parts\": [{\"text\": \"text\"}]} for user messages\n",
    "    \n",
    "    gemini_history = []\n",
    "    for msg in history:\n",
    "        role = \"model\" if msg[\"role\"] == \"assistant\" else \"user\"\n",
    "        gemini_history.append({\n",
    "            \"role\": role,\n",
    "            \"parts\": [{\"text\": msg[\"content\"]}]\n",
    "        })\n",
    "    \n",
    "    # Add the current user message\n",
    "    gemini_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"text\": message}]\n",
    "    })\n",
    "    \n",
    "    # Debugging: see what we're sending (helpful for learning!)\n",
    "    print(\"\\n=== Conversation History ===\")\n",
    "    print(f\"System Instruction: {system_message}\")\n",
    "    print(f\"Messages to Gemini:\")\n",
    "    for msg in gemini_history:\n",
    "        print(f\"  {msg['role']}: {msg['parts'][0]['text'][:50]}...\")  # First 50 chars\n",
    "    \n",
    "    # Generate streaming response with system instruction\n",
    "    response_stream = client.models.generate_content_stream(\n",
    "        model=MODEL,\n",
    "        contents=gemini_history,\n",
    "        config={\n",
    "            \"system_instruction\": system_message,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Yield chunks as they arrive\n",
    "    # This makes the text appear gradually in the interface\n",
    "    response = \"\"\n",
    "    for chunk in response_stream:\n",
    "        if chunk.text:\n",
    "            response += chunk.text\n",
    "            yield response  # Each yield updates the display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Launch Your First Chatbot!\n",
    "\n",
    "The `gr.ChatInterface()` creates a complete chat UI with:\n",
    "- Message input box\n",
    "- Conversation history display\n",
    "- Automatic handling of the conversation flow\n",
    "- Mobile-responsive design\n",
    "\n",
    "**Try it out**: \n",
    "- Ask it questions\n",
    "- Have a conversation\n",
    "- Notice how it remembers context from earlier messages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Launch the chatbot interface\n",
    "#\n",
    "# type=\"messages\" tells Gradio to use the OpenAI message format\n",
    "# (which we then convert to Gemini format in our function)\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Real-World Example: Sales Assistant\n",
    "\n",
    "Now let's build something practical - a sales assistant for a clothing store!\n",
    "\n",
    "### Business Context\n",
    "The store has a sale event:\n",
    "- Hats: 60% off\n",
    "- Most other items: 50% off\n",
    "\n",
    "### Our Goal\n",
    "Create an AI assistant that:\n",
    "1. Helps customers find items\n",
    "2. Subtly encourages them to look at sale items\n",
    "3. Especially promotes hats (highest discount)\n",
    "4. Stays helpful and not pushy\n",
    "\n",
    "### The Power of System Instructions\n",
    "Notice how we can give the AI context, examples, and behavioral guidelines all in the system message!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Enhanced system message for a sales assistant\n",
    "#\n",
    "# Notice the techniques:\n",
    "# 1. Clear role definition (\"helpful assistant in a clothes store\")\n",
    "# 2. Specific business context (sale percentages)\n",
    "# 3. Example of desired behavior\n",
    "# 4. Gentle guidance on tone (\"gently encourage\")\n",
    "\n",
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.' \\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Understanding the Chat Function (Simplified)\n",
    "\n",
    "Since our system message changed but the chat logic is the same, we can reuse the same function.\n",
    "\n",
    "**Key Insight**: By just changing the `system_message` variable, we completely changed the AI's behavior!\n",
    "\n",
    "This is the power of prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Launch with sales assistant personality\n",
    "#\n",
    "# Same code, different behavior!\n",
    "# Try asking:\n",
    "# - \"What should I buy?\"\n",
    "# - \"I need a new outfit\"\n",
    "# - \"Tell me about your hats\"\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Iterative Improvement: Adding More Context\n",
    "\n",
    "In real applications, you'll often need to refine the system message based on:\n",
    "- User feedback\n",
    "- Edge cases you discover\n",
    "- Changing business requirements\n",
    "\n",
    "Let's add handling for shoes (which aren't on sale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Iteratively improve the prompt\n",
    "#\n",
    "# We're adding to the existing system message\n",
    "# This is common - start simple, then add edge cases\n",
    "\n",
    "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
    "but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Test the improved assistant\n",
    "#\n",
    "# Try asking: \"Do you have shoes on sale?\"\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Advanced: Dynamic System Messages\n",
    "\n",
    "Sometimes you want to change the system message based on what the user asks.\n",
    "\n",
    "### Use Case: Out of Stock Items\n",
    "If someone asks about belts (which you don't sell), add that information dynamically.\n",
    "\n",
    "### Why This Matters\n",
    "- You can't predict every question\n",
    "- Some context is only relevant for certain queries\n",
    "- Keeps system message focused and efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Advanced chat function with dynamic system message\n",
    "#\n",
    "# This function modifies the system message based on the user's question\n",
    "# Notice: we create a NEW variable so we don't permanently change system_message\n",
    "\n",
    "def chat_dynamic(message, history):\n",
    "    # Start with the base system message\n",
    "    relevant_system_message = system_message\n",
    "    \n",
    "    # Add context if specific items are mentioned\n",
    "    if 'belt' in message.lower():\n",
    "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
    "    \n",
    "    # You could add more conditions:\n",
    "    # if 'return' in message.lower():\n",
    "    #     relevant_system_message += \" Our return policy is 30 days with receipt.\"\n",
    "    \n",
    "    # Convert history to Gemini format\n",
    "    gemini_history = []\n",
    "    for msg in history:\n",
    "        role = \"model\" if msg[\"role\"] == \"assistant\" else \"user\"\n",
    "        gemini_history.append({\n",
    "            \"role\": role,\n",
    "            \"parts\": [{\"text\": msg[\"content\"]}]\n",
    "        })\n",
    "    \n",
    "    gemini_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"text\": message}]\n",
    "    })\n",
    "    \n",
    "    # Generate streaming response with dynamic system instruction\n",
    "    response_stream = client.models.generate_content_stream(\n",
    "        model=MODEL,\n",
    "        contents=gemini_history,\n",
    "        config={\n",
    "            \"system_instruction\": relevant_system_message,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in response_stream:\n",
    "        if chunk.text:\n",
    "            response += chunk.text\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Test dynamic system messages\n",
    "#\n",
    "# Try:\n",
    "# - \"Do you have belts?\" (should trigger the belt message)\n",
    "# - \"What's on sale?\" (won't trigger it)\n",
    "\n",
    "gr.ChatInterface(fn=chat_dynamic, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Business Applications\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Conversational AI assistants are transforming businesses:\n",
    "\n",
    "1. **Customer Service**: 24/7 support with context awareness\n",
    "2. **Sales**: Personalized recommendations based on conversation\n",
    "3. **Education**: Tutoring that adapts to student needs\n",
    "4. **Healthcare**: Patient intake and triage\n",
    "5. **HR**: Employee onboarding and FAQ\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. ✅ Build conversational interfaces with Gradio\n",
    "2. ✅ Manage conversation history for context\n",
    "3. ✅ Use system instructions to guide AI behavior\n",
    "4. ✅ Stream responses for better UX\n",
    "5. ✅ Dynamically adapt behavior based on user input\n",
    "6. ✅ Apply to real business scenarios\n",
    "\n",
    "### Your Turn!\n",
    "\n",
    "Think about your business or a business you know:\n",
    "- What repetitive questions do customers ask?\n",
    "- What information do customers need?\n",
    "- How could an AI assistant help?\n",
    "\n",
    "Try building your own chatbot with:\n",
    "- Custom system instructions for your use case\n",
    "- Relevant business context\n",
    "- Dynamic messages for edge cases\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To make this production-ready, consider:\n",
    "1. **Error handling**: What if the API fails?\n",
    "2. **Rate limiting**: Prevent abuse\n",
    "3. **Logging**: Track conversations for improvement\n",
    "4. **Testing**: Ensure consistent behavior\n",
    "5. **Integration**: Connect to your database/CRM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Key Differences: Gemini vs OpenAI\n",
    "\n",
    "| Aspect | Gemini | OpenAI |\n",
    "|--------|---------|--------|\n",
    "| **History format** | `{\"role\": \"model\", \"parts\": \"...\"}` | `{\"role\": \"assistant\", \"content\": \"...\"}` |\n",
    "| **System message** | Via `system_instruction` parameter | As first message with role \"system\" |\n",
    "| **Streaming** | `.generate_content(stream=True)` | `.create(stream=True)` |\n",
    "| **Response access** | `chunk.text` | `chunk.choices[0].delta.content` |\n",
    "| **Model initialization** | `GenerativeModel()` for each chat | One `OpenAI()` client reused |\n",
    "\n",
    "### Why These Differences?\n",
    "\n",
    "Different AI providers have different APIs because:\n",
    "- They evolved independently\n",
    "- Different underlying architectures\n",
    "- Different design philosophies\n",
    "\n",
    "**Good news**: The concepts are the same! Once you understand one, adapting to others is straightforward.\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations! 🎉\n",
    "\n",
    "You've built a sophisticated conversational AI system using Gemini. You now understand:\n",
    "- How chat interfaces work\n",
    "- How to manage conversation context\n",
    "- How to guide AI behavior with prompts\n",
    "- How to create practical business applications\n",
    "\n",
    "This is a foundational skill in modern AI development!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment zone - build your own chatbot!\n",
    "# \n",
    "# Try creating a chatbot for:\n",
    "# - A restaurant (taking orders, dietary restrictions)\n",
    "# - A tech support desk (troubleshooting common issues)\n",
    "# - A fitness coach (workout advice, motivation)\n",
    "# - A language tutor (practice conversations)\n",
    "# \n",
    "# Start by defining your system_message here:\n",
    "\n",
    "my_system_message = \"\"\"Your custom system instructions here!\"\"\"\n",
    "\n",
    "def my_chat(message, history):\n",
    "    gemini_history = []\n",
    "    for msg in history:\n",
    "        role = \"model\" if msg[\"role\"] == \"assistant\" else \"user\"\n",
    "        gemini_history.append({\n",
    "            \"role\": role,\n",
    "            \"parts\": [{\"text\": msg[\"content\"]}]\n",
    "        })\n",
    "    \n",
    "    gemini_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"text\": message}]\n",
    "    })\n",
    "    \n",
    "    response_stream = client.models.generate_content_stream(\n",
    "        model=MODEL,\n",
    "        contents=gemini_history,\n",
    "        config={\n",
    "            \"system_instruction\": my_system_message,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in response_stream:\n",
    "        if chunk.text:\n",
    "            response += chunk.text\n",
    "            yield response\n",
    "\n",
    "# Uncomment to launch your custom chatbot:\n",
    "# gr.ChatInterface(fn=my_chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
