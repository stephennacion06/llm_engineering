{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Installation Required\n",
    "\n",
    "Before running this notebook, you need to install the Google Generative AI package. Run the cell below **only once**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Day 3 - Conversational AI with Gemini - aka Chatbot!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this notebook, you'll learn how to:\n",
    "1. **Build a conversational chatbot** using Google's Gemini model\n",
    "2. **Maintain conversation history** so the AI remembers context\n",
    "3. **Use system instructions** to guide the AI's behavior and personality\n",
    "4. **Stream responses** for a better user experience\n",
    "5. **Create a web interface** with Gradio for easy interaction\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Conversation History\n",
    "Unlike simple one-off questions, chatbots need to remember what was said before. We'll manage this history to create natural conversations.\n",
    "\n",
    "### System Instructions\n",
    "These are special instructions that guide how the AI behaves - like giving it a role or personality. Think of it as \"behind the scenes\" instructions the user doesn't see.\n",
    "\n",
    "### Streaming\n",
    "Instead of waiting for the complete response, we'll show words as they're generated - just like ChatGPT does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# \n",
    "# os: to access environment variables\n",
    "# dotenv: to load API keys from .env file\n",
    "# google.genai: Google's Gemini AI library (newer SDK)\n",
    "# gradio: for creating web interfaces\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Before we can use Gemini, we need to:\n",
    "1. Load our API key from the `.env` file\n",
    "2. Verify the key is properly set\n",
    "3. Configure the Gemini library to use our key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load and verify API key\n",
    "#\n",
    "# The load_dotenv() function reads your .env file and makes the variables available\n",
    "# We print the first 8 characters to verify it's loaded (never print the full key!)\n",
    "\n",
    "load_dotenv(override=True)\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"âœ“ Google API Key loaded successfully (begins with: {google_api_key[:8]})\")\n",
    "    # Create the Gemini client\n",
    "    client = genai.Client(api_key=google_api_key)\n",
    "else:\n",
    "    print(\"âœ— Google API Key not found - please check your .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Choose Your Model\n",
    "\n",
    "Google offers several Gemini models:\n",
    "- **gemini-2.0-flash-exp**: Latest experimental model, very fast and capable\n",
    "- **gemini-1.5-flash**: Stable, fast, and cost-effective\n",
    "- **gemini-1.5-pro**: Most capable but slower and more expensive\n",
    "\n",
    "We'll use `gemini-1.5-flash` for a good balance of speed and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize the model\n",
    "#\n",
    "# We're choosing gemini-2.0-flash-exp for:\n",
    "# - Latest features\n",
    "# - Fast responses\n",
    "# - Good quality for most tasks\n",
    "\n",
    "MODEL = 'gemini-2.0-flash-exp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## System Instructions - The AI's Personality\n",
    "\n",
    "System instructions are like giving the AI a \"role\" to play. They:\n",
    "- Define how the AI should behave\n",
    "- Set the tone and style of responses\n",
    "- Provide context about what the AI is helping with\n",
    "- Are invisible to the user\n",
    "\n",
    "**Important**: System instructions apply to the ENTIRE conversation, not just one message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define system instructions\n",
    "#\n",
    "# Start with a simple helpful assistant\n",
    "# We'll make this more sophisticated later!\n",
    "\n",
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Building the Chat Function\n",
    "\n",
    "### How Gradio ChatInterface Works\n",
    "\n",
    "Gradio's `ChatInterface` expects a function with this signature:\n",
    "```python\n",
    "def chat(message, history):\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **message**: The user's latest message (string)\n",
    "- **history**: Past conversation in Gradio format (list of dictionaries)\n",
    "\n",
    "### Gradio's History Format\n",
    "\n",
    "Gradio passes history like this:\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! How can I help?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I don't have weather data...\"}\n",
    "]\n",
    "```\n",
    "\n",
    "### Converting to Gemini Format\n",
    "\n",
    "Gemini needs a slightly different format with \"parts\":\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"user\", \"parts\": \"Hello!\"},\n",
    "    {\"role\": \"model\", \"parts\": \"Hi! How can I help?\"}  # Note: \"model\" not \"assistant\"\n",
    "]\n",
    "```\n",
    "\n",
    "### Why We Need Streaming\n",
    "\n",
    "Streaming means we show the response word-by-word as it's generated, rather than waiting for the complete answer. This:\n",
    "- Feels more responsive\n",
    "- Shows progress on long answers\n",
    "- Matches the ChatGPT experience users expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create the chat function\n",
    "#\n",
    "# This function:\n",
    "# 1. Converts Gradio's history format to Gemini's format\n",
    "# 2. Uses the Gemini client to generate responses\n",
    "# 3. Generates a streaming response\n",
    "# 4. Yields each chunk as it arrives (for gradual display)\n",
    "\n",
    "def chat(message, history):\n",
    "    # Convert Gradio format to Gemini format\n",
    "    # Gradio uses: {\"role\": \"assistant\", \"content\": \"text\"}\n",
    "    # Gemini needs: {\"role\": \"model\", \"parts\": [{\"text\": \"text\"}]} for model responses\n",
    "    # and {\"role\": \"user\", \"parts\": [{\"text\": \"text\"}]} for user messages\n",
    "    \n",
    "    gemini_history = []\n",
    "    for msg in history:\n",
    "        role = \"model\" if msg[\"role\"] == \"assistant\" else \"user\"\n",
    "        gemini_history.append({\n",
    "            \"role\": role,\n",
    "            \"parts\": [{\"text\": msg[\"content\"]}]\n",
    "        })\n",
    "    \n",
    "    # Add the current user message\n",
    "    gemini_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"text\": message}]\n",
    "    })\n",
    "    \n",
    "    # Debugging: see what we're sending (helpful for learning!)\n",
    "    print(\"\\n=== Conversation History ===\")\n",
    "    print(f\"System Instruction: {system_message}\")\n",
    "    print(f\"Messages to Gemini:\")\n",
    "    for msg in gemini_history:\n",
    "        print(f\"  {msg['role']}: {msg['parts'][0]['text'][:50]}...\")  # First 50 chars\n",
    "    \n",
    "    # Generate streaming response with system instruction\n",
    "    response_stream = client.models.generate_content_stream(\n",
    "        model=MODEL,\n",
    "        contents=gemini_history,\n",
    "        config={\n",
    "            \"system_instruction\": system_message,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Yield chunks as they arrive\n",
    "    # This makes the text appear gradually in the interface\n",
    "    response = \"\"\n",
    "    for chunk in response_stream:\n",
    "        if chunk.text:\n",
    "            response += chunk.text\n",
    "            yield response  # Each yield updates the display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Launch Your First Chatbot!\n",
    "\n",
    "The `gr.ChatInterface()` creates a complete chat UI with:\n",
    "- Message input box\n",
    "- Conversation history display\n",
    "- Automatic handling of the conversation flow\n",
    "- Mobile-responsive design\n",
    "\n",
    "**Try it out**: \n",
    "- Ask it questions\n",
    "- Have a conversation\n",
    "- Notice how it remembers context from earlier messages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Launch the chatbot interface\n",
    "#\n",
    "# type=\"messages\" tells Gradio to use the OpenAI message format\n",
    "# (which we then convert to Gemini format in our function)\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Real-World Example: Sales Assistant\n",
    "\n",
    "Now let's build something practical - a sales assistant for a clothing store!\n",
    "\n",
    "### Business Context\n",
    "The store has a sale event:\n",
    "- Hats: 60% off\n",
    "- Most other items: 50% off\n",
    "\n",
    "### Our Goal\n",
    "Create an AI assistant that:\n",
    "1. Helps customers find items\n",
    "2. Subtly encourages them to look at sale items\n",
    "3. Especially promotes hats (highest discount)\n",
    "4. Stays helpful and not pushy\n",
    "\n",
    "### The Power of System Instructions\n",
    "Notice how we can give the AI context, examples, and behavioral guidelines all in the system message!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Enhanced system message for a sales assistant\n",
    "#\n",
    "# Notice the techniques:\n",
    "# 1. Clear role definition (\"helpful assistant in a clothes store\")\n",
    "# 2. Specific business context (sale percentages)\n",
    "# 3. Example of desired behavior\n",
    "# 4. Gentle guidance on tone (\"gently encourage\")\n",
    "\n",
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.' \\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Understanding the Chat Function (Simplified)\n",
    "\n",
    "Since our system message changed but the chat logic is the same, we can reuse the same function.\n",
    "\n",
    "**Key Insight**: By just changing the `system_message` variable, we completely changed the AI's behavior!\n",
    "\n",
    "This is the power of prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Launch with sales assistant personality\n",
    "#\n",
    "# Same code, different behavior!\n",
    "# Try asking:\n",
    "# - \"What should I buy?\"\n",
    "# - \"I need a new outfit\"\n",
    "# - \"Tell me about your hats\"\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Iterative Improvement: Adding More Context\n",
    "\n",
    "In real applications, you'll often need to refine the system message based on:\n",
    "- User feedback\n",
    "- Edge cases you discover\n",
    "- Changing business requirements\n",
    "\n",
    "Let's add handling for shoes (which aren't on sale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Iteratively improve the prompt\n",
    "#\n",
    "# We're adding to the existing system message\n",
    "# This is common - start simple, then add edge cases\n",
    "\n",
    "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
    "but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Test the improved assistant\n",
    "#\n",
    "# Try asking: \"Do you have shoes on sale?\"\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Advanced: Dynamic System Messages\n",
    "\n",
    "Sometimes you want to change the system message based on what the user asks.\n",
    "\n",
    "### Use Case: Out of Stock Items\n",
    "If someone asks about belts (which you don't sell), add that information dynamically.\n",
    "\n",
    "### Why This Matters\n",
    "- You can't predict every question\n",
    "- Some context is only relevant for certain queries\n",
    "- Keeps system message focused and efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Advanced chat function with dynamic system message\n",
    "#\n",
    "# This function modifies the system message based on the user's question\n",
    "# Notice: we create a NEW variable so we don't permanently change system_message\n",
    "\n",
    "def chat_dynamic(message, history):\n",
    "    # Start with the base system message\n",
    "    relevant_system_message = system_message\n",
    "    \n",
    "    # Add context if specific items are mentioned\n",
    "    if 'belt' in message.lower():\n",
    "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
    "    \n",
    "    # You could add more conditions:\n",
    "    # if 'return' in message.lower():\n",
    "    #     relevant_system_message += \" Our return policy is 30 days with receipt.\"\n",
    "    \n",
    "    # Convert history to Gemini format\n",
    "    gemini_history = []\n",
    "    for msg in history:\n",
    "        role = \"model\" if msg[\"role\"] == \"assistant\" else \"user\"\n",
    "        gemini_history.append({\n",
    "            \"role\": role,\n",
    "            \"parts\": [{\"text\": msg[\"content\"]}]\n",
    "        })\n",
    "    \n",
    "    gemini_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"text\": message}]\n",
    "    })\n",
    "    \n",
    "    # Generate streaming response with dynamic system instruction\n",
    "    response_stream = client.models.generate_content_stream(\n",
    "        model=MODEL,\n",
    "        contents=gemini_history,\n",
    "        config={\n",
    "            \"system_instruction\": relevant_system_message,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in response_stream:\n",
    "        if chunk.text:\n",
    "            response += chunk.text\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Test dynamic system messages\n",
    "#\n",
    "# Try:\n",
    "# - \"Do you have belts?\" (should trigger the belt message)\n",
    "# - \"What's on sale?\" (won't trigger it)\n",
    "\n",
    "gr.ChatInterface(fn=chat_dynamic, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Business Applications\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Conversational AI assistants are transforming businesses:\n",
    "\n",
    "1. **Customer Service**: 24/7 support with context awareness\n",
    "2. **Sales**: Personalized recommendations based on conversation\n",
    "3. **Education**: Tutoring that adapts to student needs\n",
    "4. **Healthcare**: Patient intake and triage\n",
    "5. **HR**: Employee onboarding and FAQ\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. âœ… Build conversational interfaces with Gradio\n",
    "2. âœ… Manage conversation history for context\n",
    "3. âœ… Use system instructions to guide AI behavior\n",
    "4. âœ… Stream responses for better UX\n",
    "5. âœ… Dynamically adapt behavior based on user input\n",
    "6. âœ… Apply to real business scenarios\n",
    "\n",
    "### Your Turn!\n",
    "\n",
    "Think about your business or a business you know:\n",
    "- What repetitive questions do customers ask?\n",
    "- What information do customers need?\n",
    "- How could an AI assistant help?\n",
    "\n",
    "Try building your own chatbot with:\n",
    "- Custom system instructions for your use case\n",
    "- Relevant business context\n",
    "- Dynamic messages for edge cases\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To make this production-ready, consider:\n",
    "1. **Error handling**: What if the API fails?\n",
    "2. **Rate limiting**: Prevent abuse\n",
    "3. **Logging**: Track conversations for improvement\n",
    "4. **Testing**: Ensure consistent behavior\n",
    "5. **Integration**: Connect to your database/CRM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Key Differences: Gemini vs OpenAI\n",
    "\n",
    "| Aspect | Gemini | OpenAI |\n",
    "|--------|---------|--------|\n",
    "| **History format** | `{\"role\": \"model\", \"parts\": \"...\"}` | `{\"role\": \"assistant\", \"content\": \"...\"}` |\n",
    "| **System message** | Via `system_instruction` parameter | As first message with role \"system\" |\n",
    "| **Streaming** | `.generate_content(stream=True)` | `.create(stream=True)` |\n",
    "| **Response access** | `chunk.text` | `chunk.choices[0].delta.content` |\n",
    "| **Model initialization** | `GenerativeModel()` for each chat | One `OpenAI()` client reused |\n",
    "\n",
    "### Why These Differences?\n",
    "\n",
    "Different AI providers have different APIs because:\n",
    "- They evolved independently\n",
    "- Different underlying architectures\n",
    "- Different design philosophies\n",
    "\n",
    "**Good news**: The concepts are the same! Once you understand one, adapting to others is straightforward.\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations! ðŸŽ‰\n",
    "\n",
    "You've built a sophisticated conversational AI system using Gemini. You now understand:\n",
    "- How chat interfaces work\n",
    "- How to manage conversation context\n",
    "- How to guide AI behavior with prompts\n",
    "- How to create practical business applications\n",
    "\n",
    "This is a foundational skill in modern AI development!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment zone - build your own chatbot!\n",
    "# \n",
    "# Try creating a chatbot for:\n",
    "# - A restaurant (taking orders, dietary restrictions)\n",
    "# - A tech support desk (troubleshooting common issues)\n",
    "# - A fitness coach (workout advice, motivation)\n",
    "# - A language tutor (practice conversations)\n",
    "# \n",
    "# Start by defining your system_message here:\n",
    "\n",
    "my_system_message = \"\"\"Your custom system instructions here!\"\"\"\n",
    "\n",
    "def my_chat(message, history):\n",
    "    gemini_history = []\n",
    "    for msg in history:\n",
    "        role = \"model\" if msg[\"role\"] == \"assistant\" else \"user\"\n",
    "        gemini_history.append({\n",
    "            \"role\": role,\n",
    "            \"parts\": [{\"text\": msg[\"content\"]}]\n",
    "        })\n",
    "    \n",
    "    gemini_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"text\": message}]\n",
    "    })\n",
    "    \n",
    "    response_stream = client.models.generate_content_stream(\n",
    "        model=MODEL,\n",
    "        contents=gemini_history,\n",
    "        config={\n",
    "            \"system_instruction\": my_system_message,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in response_stream:\n",
    "        if chunk.text:\n",
    "            response += chunk.text\n",
    "            yield response\n",
    "\n",
    "# Uncomment to launch your custom chatbot:\n",
    "# gr.ChatInterface(fn=my_chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
