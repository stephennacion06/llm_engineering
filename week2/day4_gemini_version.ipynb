{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Project - Airline AI Assistant with Gemini\n",
    "\n",
    "We'll now bring together what we've learned to make an AI Customer Support assistant for an Airline using Google's Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Installation Required\n",
    "\n",
    "Make sure you have the Google Generative AI package installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Google Generative AI package\n",
    "!pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "    client = genai.Client(api_key=google_api_key)\n",
    "else:\n",
    "    print(\"Google API Key not set\")\n",
    "    \n",
    "MODEL = \"gemini-2.0-flash-exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant for an Airline called FlightAI. \"\n",
    "system_message += \"Give short, courteous answers, no more than 1 sentence. \"\n",
    "system_message += \"Always be accurate. If you don't know the answer, say so.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat function without tools first\n",
    "\n",
    "def chat(message, history):\n",
    "    # Convert Gradio format to Gemini format\n",
    "    gemini_history = []\n",
    "    for msg in history:\n",
    "        role = \"model\" if msg[\"role\"] == \"assistant\" else \"user\"\n",
    "        gemini_history.append({\n",
    "            \"role\": role,\n",
    "            \"parts\": [{\"text\": msg[\"content\"]}]\n",
    "        })\n",
    "    \n",
    "    gemini_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"text\": message}]\n",
    "    })\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL,\n",
    "        contents=gemini_history,\n",
    "        config={\n",
    "            \"system_instruction\": system_message\n",
    "        }\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Tools are an incredibly powerful feature provided by frontier LLMs.\n",
    "\n",
    "With tools (also called Function Calling), you can write a function, and have the LLM call that function as part of its response.\n",
    "\n",
    "Sounds almost spooky.. we're giving it the power to run code on our machine?\n",
    "\n",
    "Well, kinda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by making a useful function\n",
    "\n",
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "\n",
    "def get_ticket_price(destination_city: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the price of a return ticket to the destination city.\n",
    "    \n",
    "    Args:\n",
    "        destination_city: The city that the customer wants to travel to\n",
    "    \n",
    "    Returns:\n",
    "        The price of the ticket as a string\n",
    "    \"\"\"\n",
    "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ticket_price(\"Berlin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Gemini Function Calling\n",
    "\n",
    "Gemini has excellent support for function calling. We need to:\n",
    "1. Define our function (done above)\n",
    "2. Declare the function to Gemini when making a request\n",
    "3. Handle function call requests from Gemini\n",
    "4. Send the function results back to Gemini\n",
    "\n",
    "The newer Gemini SDK makes this easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini can automatically generate function declarations from Python functions!\n",
    "# We just need to pass the function itself\n",
    "\n",
    "def chat_with_tools(message, history):\n",
    "    # Convert history\n",
    "    gemini_history = []\n",
    "    for msg in history:\n",
    "        role = \"model\" if msg[\"role\"] == \"assistant\" else \"user\"\n",
    "        gemini_history.append({\n",
    "            \"role\": role,\n",
    "            \"parts\": [{\"text\": msg[\"content\"]}]\n",
    "        })\n",
    "    \n",
    "    gemini_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"text\": message}]\n",
    "    })\n",
    "    \n",
    "    # First request with tool declaration\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL,\n",
    "        contents=gemini_history,\n",
    "        config={\n",
    "            \"system_instruction\": system_message,\n",
    "            \"tools\": [get_ticket_price]  # Just pass the function!\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Check if Gemini wants to call a function\n",
    "    if response.candidates[0].content.parts[0].function_call:\n",
    "        function_call = response.candidates[0].content.parts[0].function_call\n",
    "        \n",
    "        # Extract the function name and arguments\n",
    "        function_name = function_call.name\n",
    "        function_args = dict(function_call.args)\n",
    "        \n",
    "        print(f\"Gemini wants to call: {function_name}({function_args})\")\n",
    "        \n",
    "        # Execute the function\n",
    "        if function_name == \"get_ticket_price\":\n",
    "            result = get_ticket_price(**function_args)\n",
    "            \n",
    "            # Add the function call and result to history\n",
    "            gemini_history.append({\n",
    "                \"role\": \"model\",\n",
    "                \"parts\": [{\"function_call\": function_call}]\n",
    "            })\n",
    "            gemini_history.append({\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [{\n",
    "                    \"function_response\": {\n",
    "                        \"name\": function_name,\n",
    "                        \"response\": {\"price\": result}\n",
    "                    }\n",
    "                }]\n",
    "            })\n",
    "            \n",
    "            # Get final response from Gemini\n",
    "            final_response = client.models.generate_content(\n",
    "                model=MODEL,\n",
    "                contents=gemini_history,\n",
    "                config={\n",
    "                    \"system_instruction\": system_message,\n",
    "                    \"tools\": [get_ticket_price]\n",
    "                }\n",
    "            )\n",
    "            return final_response.text\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat_with_tools, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Try asking:\n",
    "- \"How much is a ticket to London?\"\n",
    "- \"What's the price for Paris?\"\n",
    "- \"I want to fly to Tokyo\"\n",
    "\n",
    "Notice how Gemini automatically calls the function when needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Exercises and Business Applications\n",
    "\n",
    "Add in more tools - perhaps to simulate actually booking a flight. Consider:\n",
    "- A `book_flight` function\n",
    "- A `check_flight_status` function\n",
    "- A `cancel_booking` function\n",
    "\n",
    "Next: take this and apply it to your business. Make an AI assistant with tools that could carry out activities for your work. A customer support assistant? New employee onboarding assistant? So many possibilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
